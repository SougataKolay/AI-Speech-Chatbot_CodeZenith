# AI module configuration

AI_MODEL = "llama3.2:3b"

# LLM behavior controls (used in ollama.chat options)
TEMPERATURE = 0.5
TOP_P = 0.9
MAX_TOKENS = 256

# Optional timeout control
RESPONSE_TIMEOUT = 20